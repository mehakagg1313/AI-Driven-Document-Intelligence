# -*- coding: utf-8 -*-
"""Document AI Toolkit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ISPMYir_HrNbggSIBKSAxKcA2TayRRsP

# Libraries
"""

!pip install PyPDF2                 # to manipulate PDF documents by splitting, merging, cropping
!pip install pycryptodome           # for encryption, decryption, hashing
!pip install PyMuPDF                # PDF viewer -  extract text, and manipulate PDF content
!pip install pdfminer.six           # for extracting text, images, and metadata from PDF files
!pip install pdf2image              # converts PDF files into a sequence of images
!apt-get install -y tesseract-ocr
!pip install pytesseract            # Python wrapper for Google's Tesseract-OCR Engine
!pip install gTTS                   # convert text into speech using Google's voice synthesis
!pip install poppler-utils          # includes tools for converting PDFs to different formats (PDF to PNG)
!pip install transformers           # simplifying the implementation of NLP tasks by offering pre-trained models and tools
!pip install transformers sentencepiece sacremoses

# creation, reading, and extraction of ZIP archive
from zipfile import ZipFile

# for extracting text, images, and metadata from PDF files
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage

from io import StringIO

# module for encoding and decoding binary data using base64 encoding.
import base64

#------- OCR ------------
import pdf2image
import pytesseract
import fitz   # module in PyMuPDF
import os     # to interact with the os, including functions for file manipulation and directory operations
import glob   # for pattern matching files and directories

from pdf2image import convert_from_path
from pytesseract import Output, TesseractError
# from tkinter import Tk, Frame, Button, filedialog
from PyPDF2 import PdfReader
from PIL import Image         # Pillow library, working with images, including opening, manipulating, and saving images in various formats
from google.colab import files
from gtts import gTTS
from IPython.display import Audio
from transformers import pipeline
from transformers import *

import tensorflow as tf
from tensorflow.keras import datasets,layers,models
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import numpy as np
import cv2
from keras.models import load_model
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16

from sklearn.model_selection import train_test_split

import random

from google.colab import drive
drive.mount('/content/drive')

CLASSES_LIST=["aadhar","passport","driver license","pan","voter"]
IMG_SIZE=224





"""# OCR engine - Tesseract for png file"""

def png_to_text():
    folder_path = "/content/output_images"
    png_files = glob.glob(f"{folder_path}/*.png")
    extracted_text = {}

    for file_name in png_files:
        img = Image.open(file_name)
        text = pytesseract.image_to_string(img)
        extracted_text[file_name] = text

    return extracted_text

"""# Convert PDF to IMAGE

"""

def convert_pdf_to_images(pdf_path, output_folder, menu_option):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    pdf_document = fitz.open(pdf_path)

    for page_number in range(pdf_document.page_count):
        page = pdf_document.load_page(page_number)
        image_matrix = page.get_pixmap()
        img = Image.frombytes("RGB", [image_matrix.width, image_matrix.height], image_matrix.samples)
        image_path = f"{output_folder}/page_{page_number + 1}.png"
        img.save(image_path, "PNG")

    pdf_document.close()
    print(f"PDF pages converted to images in the folder: {output_folder}")

    # if classification of document ; do not need text extraction
    if(menu_option == 3):
      return

    text = png_to_text()
    return text

def convert_image_to_text(image_path, output_folder, menu_option):
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    img = Image.open(image_path)

    # Perform any necessary preprocessing or analysis here

    # Save the image as PNG
    image_filename = os.path.splitext(os.path.basename(image_path))[0]  # Get filename without extension
    output_image_path = os.path.join(output_folder, f"{image_filename}.png")
    img.save(output_image_path, "PNG")

    print(f"Image converted and saved as: {output_image_path}")

    if menu_option == 3:
        return None  # No need for text extraction if classification of document
    else:
        text = png_to_text()
        return text

"""# Uploading files"""

def upload_pdf_and_convert(menu_option):
    uploaded = files.upload()
    pdf_path = None
    image_path = None

    for name, data in uploaded.items():
        if name.endswith('.pdf'):
            with open(name, 'wb') as f:
                f.write(data)
                pdf_path = name
                print(f"Uploaded {name}")
        if name.lower().endswith(('.png', '.jpg', '.jpeg')):
            with open(name, 'wb') as f:
                f.write(data)
                image_path = name
                print(f"Uploaded {name}")

    if pdf_path:
        output_folder = 'output_images'
        text = convert_pdf_to_images(pdf_path, output_folder, menu_option)
        return text
    if image_path:
        output_folder = 'output_images'
        text = convert_image_to_text(image_path, output_folder, menu_option)
        return text
    else:
        print("\n-> Please upload a valid file.")

"""#Text to speech"""

def text_to_audio():
    print("Please upload a PDF file.")
    menu_option = 1
    extracted_text = upload_pdf_and_convert(menu_option)

    # Clean the extracted text by removing newline characters.
    cleaned_text = [text.replace('\n', ' ') for text in extracted_text.values()]
    print("Extracted text : ", cleaned_text)

    # Combine the cleaned text into a single string.
    combined_text = ' '.join(cleaned_text)

    # Using gTTS library to convert the combined text to speech.
    tts = gTTS(text=combined_text, lang='en')

    # Saved generated speech as MP3 file.
    tts.save('output.mp3')

    print("\n\n Audio has been created!\n\n")

    # Return the created audio file and autoplay it.
    return Audio('output.mp3', autoplay=True)



"""# Read Aloud Function

"""

def click_read_aloud():

    print("\nRead Aloud option chosen\n")
    text_to_audio()

"""# Document Summary Function"""

def summarize_text(text):
    summarizer = pipeline("summarization")
    summary = summarizer(text, max_length=150, min_length=30, do_sample=False)
    return summary[0]['summary_text']

def summarize_extracted_text(extracted_text):
    summarized_text = {}
    for file_name, text in extracted_text.items():
        summarized_text[file_name] = summarize_text(text)
    return summarized_text

def click_document_summary():
    print("\nDocument Summary option chosen\n")
    print("-> Please upload a PDF file.")
    menu_option = 2
    extracted_text = upload_pdf_and_convert(menu_option)

    summarized_text = summarize_extracted_text(extracted_text)

    cleaned_text = [text.replace('\n', '') for text in extracted_text]
    print("\nExtracted text : ",cleaned_text)

    for file_name, summary in summarized_text.items():
      print(f"Summary of {file_name}:\n{summary}\n{'-'*50}")

"""# Document Classification Function"""

def classify_document_images(image_folder, model):
    # Iterate through each item in the folder
    for item in os.listdir(image_folder):
        item_path = os.path.join(image_folder, item)

        # Check if the item is a file
        if os.path.isfile(item_path):
            # Load and preprocess the image
            img = image.load_img(item_path, target_size=(IMG_SIZE, IMG_SIZE))
            img_array = image.img_to_array(img)
            img_array = np.expand_dims(img_array, axis=0)
            img_array /= 255.0  # Normalize the image

            # Make a prediction
            prediction = model.predict(img_array)
            predicted_class = np.argmax(prediction)
            predicted_class_name = CLASSES_LIST[predicted_class]

            print(f"Item: {item}, Predicted Class: {predicted_class_name}")

def click_document_classification():

    print("\nDocument Classification option chosen")
    menu_option = 3
    upload_pdf_and_convert(menu_option)

    model = load_model('finalmodel.h5')

    classify_images_folder = '/content/output_images'
    classify_document_images(classify_images_folder, model)

"""#Grammar Check"""

#from language_tool_python import LanguageTool
#from happytransformer import HappyTextToText, TTSettings
from transformers import pipeline

def click_check_grammar():
    print("\nGrammar Check option chosen\n")
    print("-> Please upload a PDF file.")
    menu_option = 2
    extracted_text = upload_pdf_and_convert(menu_option)
    text_full=""
    for text in extracted_text.values():
      text_full += text

    corrected_text = grammar_checker(text_full)
    #corrected_text = grammar_checker("This are buses and these is a car.")
    print("\nCorrected text : ",corrected_text)

def grammar_checker(text):
    corrector = pipeline(
              'text2text-generation',
              'pszemraj/flan-t5-large-grammar-synthesis',
              )
    #raw_text = 'i can has cheezburger'
    results = corrector(text)
    return results

"""#qna"""

!pip install langchain
!pip install openai
!pip install PyPDF2
!pip install faiss-cpu
!pip install tiktoken

!pip install --upgrade langchain
!pip install faiss-cpu

!pip install -U langchain-community

from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS

import os
os.environ["OPENAI_API_KEY"] = "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"

# from typing_extensions import Concatenate
# # read text from pdf
# menu_option = 5
# extracted_text = upload_pdf_and_convert(menu_option)
# raw_text=""
# for content in extracted_text.values():
#     raw_text += content if content else ""

# We need to split the text using Character Text Split such that it sshould not increse token size
# text_splitter = CharacterTextSplitter(
#     separator = "\n",
#     chunk_size = 800,
#     chunk_overlap  = 200,
#     length_function = len,
# )
# texts = text_splitter.split_text(raw_text)

# embeddings = OpenAIEmbeddings()
# document_search = FAISS.from_texts(texts, embeddings)
# document_search
# from langchain.chains.question_answering import load_qa_chain
# from langchain.llms import OpenAI
# chain = load_qa_chain(OpenAI(), chain_type="stuff")
# query = input("enter your question, else type exit") #"explain the objective of the project"
# # while(True):
# #   query = input("enter your question, else type exit")
# #   if(query=="exit"):
# #     print("Thanks for exiting!")
# #     break
#   #answer = ask_question(text_full, question)
# docs = document_search.similarity_search(query)
# chain.run(input_documents=docs, question=query)

def qna():

  from typing_extensions import Concatenate
  # read text from pdf
  menu_option = 5
  extracted_text = upload_pdf_and_convert(menu_option)
  raw_text=""
  for content in extracted_text.values():
      raw_text += content if content else ""

  # We need to split the text using Character Text Split such that it sshould not increse token size
  text_splitter = CharacterTextSplitter(
      separator = "\n",
      chunk_size = 800,
      chunk_overlap  = 200,
      length_function = len,
  )
  texts = text_splitter.split_text(raw_text)

  embeddings = OpenAIEmbeddings()
  document_search = FAISS.from_texts(texts, embeddings)
  document_search
  from langchain.chains.question_answering import load_qa_chain
  from langchain.llms import OpenAI
  chain = load_qa_chain(OpenAI(), chain_type="stuff")

  while True:
      query = input("Enter your question, or type 'exit' to quit: ")
      if query.lower() == "exit":
          print("Thanks for exiting!")
          break
      docs = document_search.similarity_search(query)
      result = chain({"input_documents": docs, "question": query}, return_only_outputs=True)
      output_value = list(result.values())[0]  # Assuming there's only one value in the dictionary
      print("Answer:", output_value)  # Print the result dictionary

"""#Paraphrasing

model is BART (Bidirectional and Auto-Regressive Transformers) which has been shown to perform well on tasks like document summarization and paraphrasing.

We're using the BART model (facebook/bart-large-cnn) which is well-suited for handling longer text inputs.
We load the BART tokenizer and model.
The input text is tokenized with "paraphrase: " prepended to it to indicate to the model that it should paraphrase the input.
We generate paraphrased text using the BART model with generate() function, specifying num_beams for beam search and max_length for controlling the length of the output.
Finally, we decode the generated paraphrased text to get the final output.
"""

# from transformers import BartForConditionalGeneration, BartTokenizer

# def paraphrase_text(input_text):
#     # Load pre-trained BART model and tokenizer
#     model_name = "facebook/bart-large-cnn"
#     tokenizer = BartTokenizer.from_pretrained(model_name)
#     model = BartForConditionalGeneration.from_pretrained(model_name)

#     # Tokenize the input text
#     input_tokenized = tokenizer.encode("paraphrase: " + input_text, return_tensors="pt", max_length=1024, truncation=True)

#     # Generate paraphrased text
#     paraphrased_ids = model.generate(input_tokenized, num_beams=4, max_length=150, early_stopping=True)

#     # Decode the generated paraphrased text
#     paraphrased_text = tokenizer.decode(paraphrased_ids[0], skip_special_tokens=True)

#     return paraphrased_text

# # Example usage
# input_text = """
#     The Document Assistant is envisioned as an all-encompassing tool designed to streamline and enhance the interaction with digital documents. In the age where data is king, our tool aims to simplify complex document processing tasks, making them more efficient and user-friendly
# """
# paraphrased_text = paraphrase_text(input_text)
# print("Original text:", input_text)
# print("Paraphrased text:", paraphrased_text)

# menu_option = 5
# extracted_text = upload_pdf_and_convert(menu_option)
# from typing_extensions import Concatenate
# raw_text=""
# for text in extracted_text.values():
#   raw_text += text

# paraphrased_text = paraphrase_text(raw_text)
# print("Original text:")
# print(raw_text)
# print("\nParaphrased text:")
# print(paraphrased_text)
# lines = paraphrased_text.split('.')

# # Print each line separately
# for line in lines:
#   print(line)



""" need to maintain the length of the input document while paraphrasing, you can use a method called "chunking". This involves splitting the document into smaller chunks, paraphrasing each chunk individually, and then concatenating the paraphrased chunks to form the final paraphrased document.

"""

def paraphrase_text_final():
  from transformers import BartForConditionalGeneration, BartTokenizer

  def chunk_text(text, max_chunk_len=1024):
      """Split the text into chunks of maximum length max_chunk_len."""
      chunks = []
      current_chunk = ""
      words = text.split()
      for word in words:
          if len(current_chunk) + len(word) + 1 <= max_chunk_len:
              current_chunk += word + " "
          else:
              chunks.append(current_chunk.strip())
              current_chunk = word + " "
      if current_chunk:
          chunks.append(current_chunk.strip())
      return chunks

  def paraphrase_text_f(input_text):
      # Load pre-trained BART model and tokenizer
      model_name = "facebook/bart-large-cnn"
      tokenizer = BartTokenizer.from_pretrained(model_name)
      model = BartForConditionalGeneration.from_pretrained(model_name)

      # Chunk the input text
      text_chunks = chunk_text(input_text)

      # Paraphrase each chunk
      paraphrased_chunks = []
      for chunk in text_chunks:
          input_tokenized = tokenizer.encode("paraphrase: " + chunk, return_tensors="pt", max_length=1024, truncation=True)
          paraphrased_ids = model.generate(input_tokenized, num_beams=4, max_length=150, early_stopping=True)
          paraphrased_text = tokenizer.decode(paraphrased_ids[0], skip_special_tokens=True)
          paraphrased_chunks.append(paraphrased_text)

      # Concatenate paraphrased chunks
      paraphrased_text = " ".join(paraphrased_chunks)

      return paraphrased_text

  menu_option = 5
  extracted_text = upload_pdf_and_convert(menu_option)
  from typing_extensions import Concatenate
  raw_text=""
  for text in extracted_text.values():
    raw_text += text

  paraphrased_text = paraphrase_text_f(raw_text)
  # print("Original text:")
  # print(raw_text)
  # print("\nParaphrased text:")
  # print(paraphrased_text)
  lines = paraphrased_text.split('.')

  # Print each line separately
  for line in lines:
    print(line)
paraphrase_text_final()

import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# Download the 'punkt' resource for tokenization
nltk.download('punkt')

def calculate_bleu(reference_text, generated_text):
    # Tokenize the reference and generated texts
    reference_tokens = [nltk.word_tokenize(reference_text)]
    generated_tokens = nltk.word_tokenize(generated_text)

    # Use smoothing function for BLEU score calculation
    smoothing_function = SmoothingFunction().method1

    # Calculate BLEU score
    bleu_score = sentence_bleu(reference_tokens, generated_tokens, smoothing_function=smoothing_function)

    return bleu_score

# Example usage
reference = "The quick brown fox jumps over the lazy dog."
generated = "A fast brown fox leaps over a lazy dog."

bleu_score = calculate_bleu(reference, generated)
print(f"BLEU Score: {bleu_score:.4f}")

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
import random

nltk.download('punkt')
nltk.download('wordnet')

#paraphrase_text function takes the extracted text from the PDF as input, tokenizes it into words, and then finds synonyms for each word using WordNet, which is a lexical database of semantic relations between words in more than 200 languages. Finally, it constructs a paraphrased text by randomly choosing synonyms for each word.

# def paraphrase_text(text):
#     tokens = word_tokenize(text)
#     lemmatizer = WordNetLemmatizer()

#     paraphrased_text = []

#     for token in tokens:
#         synonyms = []
#         for syn in wordnet.synsets(token):
#             for lemma in syn.lemmas():
#                 synonyms.append(lemma.name())
#         if synonyms:
#             paraphrased_text.append(random.choice(synonyms))
#         else:
#             paraphrased_text.append(token)

#     return ' '.join(paraphrased_text)

# # Example usage
# #text = " The Document Assistant is envisioned as an all-encompassing tool designed to streamline and enhance the interaction with digital documents. In the age where data is king, our tool aims to simplify complex document processing tasks, making them more efficient and user-friendly"

# menu_option = 5
# extracted_text = upload_pdf_and_convert(menu_option)
# from typing_extensions import Concatenate
# raw_text=""
# for text in extracted_text.values():
#   raw_text += text

# paraphrased_text = paraphrase_text(raw_text)
# print("Original text:")
# print(raw_text)
# print("\nParaphrased text:")
# lines = paraphrased_text.split('.')

# # Print each line separately
# for line in lines:
#     print(line)

# #print(paraphrased_text)

from transformers import PegasusForConditionalGeneration, PegasusTokenizer

def paraphrase_text(input_text):
    # Load the Pegasus model and tokenizer
    model_name = "tuner007/pegasus_paraphrase"
    tokenizer = PegasusTokenizer.from_pretrained(model_name)
    model = PegasusForConditionalGeneration.from_pretrained(model_name)

    # Tokenize the input text
    inputs = tokenizer([input_text], max_length=1024, truncation=True, return_tensors="pt")

    # Generate paraphrased text
    paraphrased_ids = model.generate(inputs["input_ids"], max_length=1024, num_beams=5, early_stopping=True)
    paraphrased_text = tokenizer.decode(paraphrased_ids[0], skip_special_tokens=True)

    return paraphrased_text

def paraphrase_extracted_text(extracted_text):
    paraphrased_text = {}
    for file_name, text in extracted_text.items():
        paraphrased_text[file_name] = paraphrase_text(text)
    return paraphrased_text

def click_paraphrase():
    print("\nParaphrase option chosen")
    menu_option = 5  # Assuming this is the menu option for paraphrasing
    extracted_text = upload_pdf_and_convert(menu_option)

    paraphrased_text = paraphrase_extracted_text(extracted_text)

    cleaned_text = [text.replace('\n', '') for text in extracted_text]
    print("\nExtracted text : ", cleaned_text)

    for file_name, paraphrased in paraphrased_text.items():
        print(f"Paraphrased version of {file_name}:\n{paraphrased}\n{'-'*50}")

from transformers import PegasusForConditionalGeneration, PegasusTokenizer

def paraphrase_text(input_text):
    # Load the Pegasus model and tokenizer
    model_name = "tuner007/pegasus_paraphrase"
    tokenizer = PegasusTokenizer.from_pretrained(model_name)
    model = PegasusForConditionalGeneration.from_pretrained(model_name)

    # Tokenize the input text
    inputs = tokenizer([input_text], max_length=1024, truncation=True, return_tensors="pt")

    # Generate paraphrased text
    paraphrased_ids = model.generate(inputs["input_ids"], max_length=1024, num_beams=5, early_stopping=True)
    paraphrased_text = tokenizer.decode(paraphrased_ids[0], skip_special_tokens=True)

    return paraphrased_text

def paraphrase_extracted_text(extracted_text):
    paraphrased_text = {}
    for file_name, text in extracted_text.items():
        paraphrased_text[file_name] = paraphrase_text(text)
    return paraphrased_text

def click_paraphrase():
    print("\nParaphrase option chosen")
    menu_option = 5  # Assuming this is the menu option for paraphrasing
    extracted_text = upload_pdf_and_convert(menu_option)

    paraphrased_text = paraphrase_extracted_text(extracted_text)

    cleaned_text = [text.replace('\n', '') for text in extracted_text]
    print("\nExtracted text : ", cleaned_text)

    for file_name, paraphrased in paraphrased_text.items():
        print(f"Paraphrased version of {file_name}:\n{paraphrased}\n{'-'*50}")

# Assume the rest of your code remains the same

def main_menu():
    print("Welcome to the Main Menu")
    print("1. Read Aloud")
    print("2. Document Summary")
    print("3. Document Classification")
    print("4. Exit")
    print("5. Paraphrase")

    while True:
        try:
            option = int(input("Please enter the option number: "))
            if option == 1:
                click_read_aloud()
            elif option == 2:
                click_document_summary()
            elif option == 3:
                click_document_classification()
            elif option == 4:
                print("Exiting the program. Goodbye!")
                break  # Exit the loop
            elif option == 5:
                click_paraphrase()
            else:
                print("\n # Invalid option. Please enter a number between 1 and 5.")
        except ValueError:
            print("\n # Invalid input. Please enter a number.")

# Calling the menu function
main_menu()

from transformers import PegasusForConditionalGeneration, PegasusTokenizer

def paraphrase_text(input_text):
    # Load the Pegasus model and tokenizer
    model_name = "tuner007/pegasus_paraphrase"
    tokenizer = PegasusTokenizer.from_pretrained(model_name)
    model = PegasusForConditionalGeneration.from_pretrained(model_name)

    # Tokenize the input text
    inputs = tokenizer([input_text], max_length=1024, truncation=True, return_tensors="pt")

    # Generate paraphrased text
    paraphrased_ids = model.generate(inputs["input_ids"], max_length=256, num_beams=5, early_stopping=True)
    paraphrased_text = tokenizer.decode(paraphrased_ids[0], skip_special_tokens=True)

    return paraphrased_text

def paraphrase_extracted_text(extracted_text):
    paraphrased_text = {}
    for file_name, text in extracted_text.items():
        paraphrased_text[file_name] = paraphrase_text(text)
    return paraphrased_text

def click_paraphrase():
    print("\nParaphrase option chosen")
    menu_option = 5  # Assuming this is the menu option for paraphrasing
    extracted_text = upload_pdf_and_convert(menu_option)

    if extracted_text:
        paraphrased_text = paraphrase_extracted_text(extracted_text)

        cleaned_text = [text.replace('\n', '') for text in extracted_text.values()]
        print("\nExtracted text : ", cleaned_text)

        for file_name, paraphrased in paraphrased_text.items():
            print(f"Paraphrased version of {file_name}:\n{paraphrased}\n{'-'*50}")

def main_menu():
    print("Welcome to the Main Menu")
    print("1. Read Aloud")
    print("2. Document Summary")
    print("3. Document Classification")
    print("4. Exit")
    print("5. Paraphrase")

    while True:
        try:
            option = int(input("Please enter the option number: "))
            if option == 1:
                click_read_aloud()
            elif option == 2:
                click_document_summary()
            elif option == 3:
                click_document_classification()
            elif option == 4:
                print("Exiting the program. Goodbye!")
                break  # Exit the loop
            elif option == 5:
                click_paraphrase()
            else:
                print("\n # Invalid option. Please enter a number between 1 and 5.")
        except ValueError:
            print("\n # Invalid input. Please enter a number.")

# Calling the menu function
main_menu()



from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

def paraphrase_text(input_text):
    # Load the paraphrasing model
    tokenizer = AutoTokenizer.from_pretrained("tuner007/pegasus_paraphrase")
    model = AutoModelForSeq2SeqLM.from_pretrained("tuner007/pegasus_paraphrase")

    # Tokenize the input text
    inputs = tokenizer(input_text, return_tensors="pt", max_length=1024, truncation=True)

    # Generate paraphrased text
    paraphrased_ids = model.generate(inputs["input_ids"], max_length=1024, num_return_sequences=1, early_stopping=True)
    paraphrased_text = tokenizer.decode(paraphrased_ids[0], skip_special_tokens=True)

    return paraphrased_text

def paraphrase_extracted_text(extracted_text):
    paraphrased_text = {}
    for file_name, text in extracted_text.items():
        paraphrased_text[file_name] = paraphrase_text(text)
    return paraphrased_text

def click_paraphrase():
    print("\nParaphrase option chosen")
    menu_option = 4
    extracted_text = upload_pdf_and_convert(menu_option)

    paraphrased_text = paraphrase_extracted_text(extracted_text)

    cleaned_text = [text.replace('\n', '') for text in extracted_text]
    print("\nExtracted text : ", cleaned_text)

    for file_name, paraphrased in paraphrased_text.items():
        print(f"Paraphrased version of {file_name}:\n{paraphrased}\n{'-'*50}")

def main_menu():
    print("Welcome to the Main Menu")
    print("1. Read Aloud")
    print("2. Document Summary")
    print("3. Document Classification")
    print("4. Paraphrase")
    print("5. Exit")

    while True:
        try:
            option = int(input("Please enter the option number: "))
            if option == 1:
                click_read_aloud()
            elif option == 2:
                click_document_summary()
            elif option == 3:
                click_document_classification()
            elif option == 4:
                click_paraphrase()
            elif option == 5:
                exit_menu_page()
                break  # Exit the loop
            else:
                print("\n # Invalid option. Please enter a number between 1 and 5.")
        except ValueError:
            print("\n # Invalid input. Please enter a number.")

# Calling the menu function
main_menu()

import torch
from transformers import BartForConditionalGeneration, BartTokenizer

# Load BART model and tokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

def paraphrase_text(text):
    input_ids = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)
    paraphrased_ids = model.generate(input_ids, num_return_sequences=1, max_length=150, early_stopping=True)
    paraphrased_text = tokenizer.decode(paraphrased_ids[0], skip_special_tokens=True)
    return paraphrased_text

# Your existing code for document processing functions goes here...

# Modify main menu function to include paraphrasing option
def main_menu():
    print("Welcome to the Main Menu")
    print("1. Read Aloud")
    print("2. Document Summary")
    print("3. Document Classification")
    print("4. Exit")
    print("5. Paraphrase")

    while True:
        try:
            option = int(input("Please enter the option number: "))
            if option == 1:
                click_read_aloud()
            elif option == 2:
                click_document_summary()
            elif option == 3:
                click_document_classification()
            elif option == 4:
                exit_menu_page()
                break  # Exit the loop
            elif option == 5:
                paraphrase_menu()
            else:
                print("\n # Invalid option. Please enter a number between 1 and 5.")
        except ValueError:
            print("\n # Invalid input. Please enter a number.")

# Add function for paraphrasing menu
def paraphrase_menu():
    print("\nParaphrase option chosen\n")
    menu_option = 1  # Assuming we always use the extracted text for paraphrasing
    extracted_text = upload_pdf_and_convert(menu_option)

    cleaned_text = [text.replace('\n', ' ') for text in extracted_text.values()]
    combined_text = ' '.join(cleaned_text)

    paraphrased_text = paraphrase_text(combined_text)
    print("\nParaphrased text:\n", paraphrased_text)

# Your existing code for other functions goes here...

# Calling the menu function
main_menu()

# Import necessary modules for paraphrasing
from transformers import pipeline

# Define function for paraphrasing
def paraphrase_text(text):
    paraphraser = pipeline("text2text-generation", model="t5-small", tokenizer="t5-small")
    paraphrased_text = paraphraser(text, max_length=1000, num_return_sequences=1)[0]['generated_text']
    return paraphrased_text

# Modify main menu function to include paraphrasing option
def main_menu():
    print("Welcome to the Main Menu")
    print("1. Read Aloud")
    print("2. Document Summary")
    print("3. Document Classification")
    print("4. Exit")
    print("5. Paraphrase")

    while True:
        try:
            option = int(input("Please enter the option number: "))
            if option == 1:
                click_read_aloud()
            elif option == 2:
                click_document_summary()
            elif option == 3:
                click_document_classification()
            elif option == 4:
                exit_menu_page()
                break  # Exit the loop
            elif option == 5:
                paraphrase_menu()
            else:
                print("\n # Invalid option. Please enter a number between 1 and 5.")
        except ValueError:
            print("\n # Invalid input. Please enter a number.")

# Add function for paraphrasing menu
def paraphrase_menu():
    print("\nParaphrase option chosen\n")
    menu_option = 1  # Assuming we always use the extracted text for paraphrasing
    extracted_text = upload_pdf_and_convert(menu_option)

    cleaned_text = [text.replace('\n', ' ') for text in extracted_text.values()]
    combined_text = ' '.join(cleaned_text)

    paraphrased_text = paraphrase_text(combined_text)
    print("\nParaphrased text:\n", paraphrased_text)

# Calling the menu function
main_menu()

from transformers import PegasusForConditionalGeneration, PegasusTokenizerFast

# Initialize the Pegasus model and tokenizer
model = PegasusForConditionalGeneration.from_pretrained("tuner007/pegasus_paraphrase")
tokenizer = PegasusTokenizerFast.from_pretrained("tuner007/pegasus_paraphrase")

def get_paraphrased_sentences(sentence, num_return_sequences=5, num_beams=5):
    # Generate paraphrased sentences
    inputs = tokenizer([sentence], truncation=True, padding="longest", return_tensors="pt")
    outputs = model.generate(
        **inputs,
        num_beams=num_beams,
        num_return_sequences=num_return_sequences,
        early_stopping=True
    )
    # Decode the generated sentences using the tokenizer
    return tokenizer.batch_decode(outputs, skip_special_tokens=True)

def paraphrase_sentences(sentences):
    paraphrased_sentences = []
    for sentence in sentences:
        paraphrased_sentences.extend(get_paraphrased_sentences(sentence))
    return paraphrased_sentences

def final_paraphrased():
    menu_option = 5
    extracted_text = upload_pdf_and_convert(menu_option)
    cleaned_text = [text.replace('\n', '') for text in extracted_text]
    print("\nExtracted text : ", cleaned_text)

    # Paraphrase the extracted sentences
    paraphrased_text = paraphrase_sentences(cleaned_text)
    return paraphrased_text

# from transformers import PegasusForConditionalGeneration, PegasusTokenizerFast
# import pytesseract
# from PIL import Image

# # Initialize the Pegasus model and tokenizer
# model = PegasusForConditionalGeneration.from_pretrained("tuner007/pegasus_paraphrase")
# tokenizer = PegasusTokenizerFast.from_pretrained("tuner007/pegasus_paraphrase")

# def perform_ocr(pdf_path):
#     images = convert_pdf_to_images(pdf_path)
#     extracted_text = []
#     for img_path in images:
#         extracted_text.append(pytesseract.image_to_string(Image.open(img_path)))
#     return ' '.join(extracted_text)

# def get_paraphrased_sentences(sentence, num_return_sequences=5, num_beams=5):
#     # Generate paraphrased sentences
#     inputs = tokenizer([sentence], truncation=True, padding="longest", return_tensors="pt")
#     outputs = model.generate(
#         **inputs,
#         num_beams=num_beams,
#         num_return_sequences=num_return_sequences,
#         early_stopping=True
#     )
#     # Decode the generated sentences using the tokenizer
#     return tokenizer.batch_decode(outputs, skip_special_tokens=True)

# def paraphrase_sentences(sentences):
#     paraphrased_sentences = []
#     for sentence in sentences:
#         paraphrased_sentences.extend(get_paraphrased_sentences(sentence))
#     return paraphrased_sentences

# def final_paraphrased():
#     menu_option = 5
#     pdf_path = "example.pdf"  # Update with your PDF path
#     extracted_text = perform_ocr(pdf_path)
#     print("\nExtracted text : ", extracted_text)

#     # Paraphrase the extracted text
#     paraphrased_text = paraphrase_sentences([extracted_text])
#     return paraphrased_text

# # Update main_menu() function
# def main_menu():
#     print("Welcome to the Main Menu")
#     print("1. Read Aloud")
#     print("2. Document Summary")
#     print("3. Document Classification")
#     print("4. Exit")
#     print("5. Paraphrase")

#     while True:
#         try:
#             option = int(input("Please enter the option number: "))
#             if option == 1:
#                 click_read_aloud()
#             elif option == 2:
#                 click_document_summary()
#             elif option == 3:
#                 click_document_classification()
#             elif option == 4:
#                 exit_menu_page()
#                 break  # Exit the loop
#             elif option == 5:
#                 paraphrased_text = final_paraphrased()
#                 print("Paraphrased Text:")
#                 for sentence in paraphrased_text:
#                     print(sentence)
#             else:
#                 print("\n # Invalid option. Please enter a number between 1 and 5.")
#         except ValueError:
#             print("\n # Invalid input. Please enter a number.")

# # Calling the menu function
# main_menu()



"""# Main Menu"""



# Update main_menu() function
def main_menu():
    print("Welcome to the Main Menu")
    print("1. Read Aloud")
    print("2. Document Summary")
    print("3. Document Classification")
    print("4. Question Answering")
    print("5. Paraphrase")
    print("6. Grammar Check")
    print("7. Exit")

    while True:
        try:
            option = int(input("Please enter the option number: "))
            if option == 1:
                click_read_aloud()
            elif option == 2:
                click_document_summary()
            elif option == 3:
                click_document_classification()
            elif option == 4:
                qna()
            elif option == 5:
                paraphrase_text_final()
            elif option == 6:
                click_check_grammar()
            elif option == 7:
                exit_menu_page()
                break  # Exit the loop
            else:
                print("\n # Invalid option. Please enter a number between 1 and 5.")
        except ValueError:
            print("\n # Invalid input. Please enter a number.")
def exit_menu_page():
    print("Exiting!!")

# Calling the menu function
main_menu()

!pip install -U langchain-openai
from langchain_openai import OpenAIEmbeddings

from difflib import SequenceMatcher

def calculate_similarity(str1, str2):
    return SequenceMatcher(None, str1, str2).ratio()

def calculate_accuracy(ground_truth_answers, generated_answers, confidence_scores):
    total_questions = len(ground_truth_answers)
    correct_answers = 0
    confidence_per_question = {}

    for question, ground_truth_answer in ground_truth_answers.items():
        generated_answer = generated_answers.get(question, None)
        confidence_score = confidence_scores.get(question, None)

        if generated_answer:
            similarity_score = calculate_similarity(generated_answer.lower(), ground_truth_answer.lower())
            print("Similarity Score:", similarity_score)
            if similarity_score >= 0:  # Adjust the threshold as needed
                correct_answers += 1
                if confidence_score is not None:
                    confidence_per_question[question] = confidence_score

    accuracy = (correct_answers / total_questions) * 100
    return accuracy, confidence_per_question


# Example usage:
# (Assuming the rest of the code remains the same)

ground_truth_answers = {}
generated_answers = {}
confidence_scores = {}

# Get user-generated data
num_questions = int(input("Enter the number of questions: "))
for i in range(num_questions):
    question = input("Enter the question: ")
    ground_truth_answer = input("Enter the ground truth answer: ")
    generated_answer = input("Enter the generated answer: ")
    # Generate confidence score (example: randomly between 0 and 1)
    confidence_score = random.uniform(0, 1)

    ground_truth_answers[question] = ground_truth_answer
    generated_answers[question] = generated_answer
    confidence_scores[question] = confidence_score

# Calculate accuracy and confidence score for each question
accuracy, confidence_per_question = calculate_accuracy(ground_truth_answers, generated_answers, confidence_scores)
print("Accuracy: {:.2f}%".format(accuracy))
print("Confidence Scores:")
for question, confidence_score in confidence_per_question.items():
    print("- Question:", question)
    print("  Confidence Score:", confidence_score)

!pip install sentence-transformers
from sentence_transformers import SentenceTransformer
import numpy as np

def calculate_semantic_similarity(str1, str2):
    """
    Calculate semantic similarity score between two sentences using BERT embeddings.
    """
    model = SentenceTransformer('distilbert-base-nli-mean-tokens')
    embeddings1 = model.encode([str1])
    embeddings2 = model.encode([str2])
    cosine_sim = np.dot(embeddings1[0], embeddings2[0]) / (np.linalg.norm(embeddings1[0]) * np.linalg.norm(embeddings2[0]))
    return cosine_sim

def calculate_confidence_score(generated_answer, ground_truth_answer):
    """
    Calculate confidence score based on semantic similarity between generated and ground truth answers.
    """
    similarity_score = calculate_semantic_similarity(generated_answer.lower(), ground_truth_answer.lower())
    confidence_score = (similarity_score + 1) * 50  # Scaling cosine similarity score to range [0, 100]
    return confidence_score


# Example usage:
ground_truth_answers = {}
generated_answers = {}

# Get user-generated data
num_questions = int(input("Enter the number of questions: "))
for i in range(num_questions):
    question = input("Enter the question: ")
    ground_truth_answer = input("Enter the ground truth answer: ")
    generated_answer = input("Enter the generated answer: ")

    ground_truth_answers[question] = ground_truth_answer
    generated_answers[question] = generated_answer

# Calculate confidence score for each question
confidence_scores = {}
for question, generated_answer in generated_answers.items():
    ground_truth_answer = ground_truth_answers.get(question, "")
    confidence_score = calculate_confidence_score(generated_answer, ground_truth_answer)
    confidence_scores[question] = confidence_score

# Print confidence scores
print("Confidence Scores:")
for question, confidence_score in confidence_scores.items():
    print("- Question:", question)
    print("  Confidence Score:", confidence_score)

!pip show google-colab

